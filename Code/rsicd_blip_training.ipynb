{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"arampacha/rsicd\") # HF dataset for rsicd","metadata":{"execution":{"iopub.status.busy":"2025-11-16T16:34:07.405471Z","iopub.execute_input":"2025-11-16T16:34:07.405824Z","iopub.status.idle":"2025-11-16T16:34:31.222704Z","shell.execute_reply.started":"2025-11-16T16:34:07.405798Z","shell.execute_reply":"2025-11-16T16:34:31.221651Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"dataset_infos.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66a48f95558f4f5eb70d33f38081bb70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/419M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"984b3a645ffd4f53bef18be8504d2bb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/55.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd435dbbac094d3fbced57912b57baa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/valid-00000-of-00001.parquet:   0%|          | 0.00/51.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e4ad5b29b31424d936e7eeed88e5e12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/8734 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d027407e5c694abdb71224fb9ee97596"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1093 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6482d598a284a338c8380473571e7c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating valid split:   0%|          | 0/1094 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f95baae77ba441ea9488a027d44b357"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"from PIL import Image\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom accelerate import Accelerator, notebook_launcher\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2025-11-16T16:34:31.224059Z","iopub.execute_input":"2025-11-16T16:34:31.224627Z","iopub.status.idle":"2025-11-16T16:34:37.553273Z","shell.execute_reply.started":"2025-11-16T16:34:31.224595Z","shell.execute_reply":"2025-11-16T16:34:37.552333Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class CustomDataset(Dataset):\n    def __init__(self, dataset, processor):\n        self.dataset = dataset\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        # remove batch dimension\n        encodings = []\n        for caption in item[\"captions\"]:\n            encoding = self.processor(images=item[\"image\"], text=caption, padding=\"max_length\", return_tensors=\"pt\")\n            encoding = {k:v.squeeze() for k,v in encoding.items()}\n            encodings.append(encoding)\n        return encodings","metadata":{"execution":{"iopub.status.busy":"2025-11-16T16:34:37.554126Z","iopub.execute_input":"2025-11-16T16:34:37.554649Z","iopub.status.idle":"2025-11-16T16:34:37.560956Z","shell.execute_reply.started":"2025-11-16T16:34:37.554621Z","shell.execute_reply":"2025-11-16T16:34:37.559985Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom torch.optim import AdamW \nfrom transformers.optimization import get_linear_schedule_with_warmup\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau","metadata":{"execution":{"iopub.status.busy":"2025-11-16T16:34:37.563089Z","iopub.execute_input":"2025-11-16T16:34:37.563372Z","iopub.status.idle":"2025-11-16T16:35:09.785580Z","shell.execute_reply.started":"2025-11-16T16:34:37.563349Z","shell.execute_reply":"2025-11-16T16:35:09.784593Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-11-16 16:34:44.063744: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763310884.481403      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763310884.596539      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"# Initialize the tokenizer, processor, and model\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")","metadata":{"execution":{"iopub.status.busy":"2025-11-16T16:35:09.786246Z","iopub.execute_input":"2025-11-16T16:35:09.786882Z","iopub.status.idle":"2025-11-16T16:35:11.868178Z","shell.execute_reply.started":"2025-11-16T16:35:09.786854Z","shell.execute_reply":"2025-11-16T16:35:11.867359Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b96a337964b4326b01a771bbd7eaf7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b507076f68941ea9bb825f1a7513ee9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e059ccbdf0c4336a1d9de60558f0490"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0074507f511140f9b9cec536e0f10399"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e498e0ac6633410bb34a52fa46f86631"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"train_dataset = CustomDataset(dataset[\"train\"], processor)\nval_dataset = CustomDataset(dataset[\"valid\"], processor)","metadata":{"execution":{"iopub.status.busy":"2025-11-16T16:35:11.868884Z","iopub.execute_input":"2025-11-16T16:35:11.869118Z","iopub.status.idle":"2025-11-16T16:35:11.873420Z","shell.execute_reply.started":"2025-11-16T16:35:11.869096Z","shell.execute_reply":"2025-11-16T16:35:11.872680Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def training_loop(mixed_precision=\"fp16\", num_epochs=3, learning_rate=5e-5):\n    # Initialize accelerator\n    accelerator = Accelerator(mixed_precision=mixed_precision)\n    \n    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n    \n    # Use DataLoader for efficient batching\n    train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False)\n    \n    # Set up the optimizer and learning rate scheduler\n    optimizer = AdamW(model.parameters(), lr=learning_rate)\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n    \n    model, optimizer, train_loader, val_loader = accelerator.prepare(model, optimizer, train_loader, val_loader)\n    \n    model.train()\n    for epoch in range(num_epochs):\n        epoch_losses = []  # To store losses for each batch in the epoch\n\n        for idx, encodings in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", unit=\"batch\")):\n            for encoding in encodings:\n                input_ids = encoding.pop(\"input_ids\")\n                pixel_values = encoding.pop(\"pixel_values\")\n\n                outputs = model(input_ids=input_ids,\n                                pixel_values=pixel_values,\n                                labels=input_ids)\n\n                loss = outputs.loss\n                epoch_losses.append(loss.item())  # Store the loss for this batch\n\n                accelerator.backward(loss)\n\n                optimizer.step()\n                optimizer.zero_grad()\n\n        # Calculate and print the average loss for the epoch\n        average_loss = sum(epoch_losses) / len(epoch_losses)\n        accelerator.print(f\"Average Training Loss for Epoch {epoch + 1}: {average_loss}\")\n\n        # Validation phase\n        model.eval()\n        val_losses = []\n\n        with torch.no_grad():\n            for val_encodings in tqdm(val_loader, desc=\"Validation\", unit=\"batch\"):\n                for val_encoding in val_encodings:\n                    val_input_ids = val_encoding.pop(\"input_ids\")\n                    val_pixel_values = val_encoding.pop(\"pixel_values\")\n\n                    val_outputs = model(input_ids=val_input_ids,\n                                        pixel_values=val_pixel_values,\n                                        labels=val_input_ids)\n\n                    val_loss = val_outputs.loss\n                    val_losses.append(val_loss.item())\n\n        average_val_loss = sum(val_losses) / len(val_losses)\n        accelerator.print(f\"Average Validation Loss for Epoch {epoch + 1}: {average_val_loss}\")\n\n        # Update learning rate based on validation loss\n        scheduler.step(average_val_loss)\n\n        model.train()\n        \n        save_path = f\"kaggle/working/new_model_epoch_{epoch + 1}\"\n        \n    # Save the fine-tuned model\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            save_path,\n            is_main_process=accelerator.is_main_process,\n            save_function=accelerator.save,\n        )","metadata":{"execution":{"iopub.status.busy":"2025-11-16T16:35:11.874265Z","iopub.execute_input":"2025-11-16T16:35:11.874778Z","iopub.status.idle":"2025-11-16T16:35:11.899051Z","shell.execute_reply.started":"2025-11-16T16:35:11.874746Z","shell.execute_reply":"2025-11-16T16:35:11.898175Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# import torch\n\n# # This is the critical line to resolve the CUDA re-initialization error\n# try:\n#     torch.multiprocessing.set_start_method('spawn', force=True)\n# except RuntimeError as e:\n#     print(f\"Start method already set or error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T16:35:11.899933Z","iopub.execute_input":"2025-11-16T16:35:11.900215Z","iopub.status.idle":"2025-11-16T16:35:11.921276Z","shell.execute_reply.started":"2025-11-16T16:35:11.900186Z","shell.execute_reply":"2025-11-16T16:35:11.920355Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"args = (\"fp16\", 5, 5e-7)\nnotebook_launcher(training_loop, args, num_processes=1)","metadata":{"execution":{"iopub.status.busy":"2025-11-16T16:35:11.922328Z","iopub.execute_input":"2025-11-16T16:35:11.922704Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Launching training on CPU.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83efe08df4184d6980617f64dd1e373a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d810358b82ff43ccaa128da5ca0fe50c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f8ecea9d7e94faeb327e4592600fad3"}},"metadata":{}},{"name":"stderr","text":"\nEpoch 1:   0%|          | 0/1747 [00:00<?, ?batch/s]\u001b[AWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}